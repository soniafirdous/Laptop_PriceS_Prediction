# ğŸ’» Laptop Price Prediction

This project aims to predict laptop prices based on their specifications using regression models. 

The dataset includes detailed features such as company, processor, memory, GPU, operating system, and display characteristics.

A logarithmic transformation was applied to the target variable (price) to handle skewness and reduce the impact of extreme values.

ğŸ“Œ Dataset Description

Key features in the dataset:

Company â†’ Laptop manufacturer

TypeName â†’ Laptop type (Ultrabook, Notebook, Gaming, etc.)

Inches â†’ Screen size in inches

ScreenResolution â†’ Display resolution

Cpu â†’ Full CPU name

Ram â†’ System memory (GB)

Memory â†’ Storage information (HDD/SSD)

Gpu â†’ Graphics card

OpSys â†’ Operating system

Weight â†’ Laptop weight (kg)

Touchscreen â†’ Whether the laptop has a touchscreen (0/1)

IPS â†’ IPS display indicator (0/1)

PPI â†’ Pixels per inch (calculated from resolution and screen size)

Cpu_brand â†’ Extracted CPU brand (Intel, AMD, etc.)

HDD / SSD â†’ Separated storage features

Gpu brand â†’ Extracted GPU brand

OS â†’ Simplified operating system category

Price (target) â†’ Laptop price (log-transformed for modeling)

ğŸ¯ Objective

Predict laptop prices based on specifications.

Compare Linear Regression, Ridge, and Lasso models after applying log transformation.

Evaluate using RÂ², MAE, and RMSE.

ğŸ“Š Model Performance (Test Set)

Model	RÂ²	MAE	RMSE

Linear Regression	0.813	0.209	â€”

Lasso Regression (Î±=0.001)	0.807	0.211	0.272

Ridge Regression (Best Î±)	0.807	0.210	0.271

Linear Regression performed best with RÂ² = 0.813 and lowest MAE.

Ridge and Lasso gave nearly identical results (RÂ² â‰ˆ 0.807).

MAE â‰ˆ 0.21 (log scale) â†’ Back-transformed (exp(0.21) â‰ˆ 1.23), predictions are on average about 23% higher or lower than actual laptop prices.

ğŸ” Interpretation

Log transformation improved error stability and reduced the effect of very expensive laptops.

Regularization (Ridge, Lasso) did not significantly improve results, indicating no severe multicollinearity.

Lasso is still useful for feature selection and interpretability.

Key predictive features: Company, RAM, CPU brand, GPU brand, storage (SSD/HDD), and display quality (PPI, touchscreen, IPS).

ğŸš€ Next Steps

Explore feature interactions (e.g., RAM Ã— GPU, CPU Ã— SSD).

Experiment with non-linear models (Random Forest, Gradient Boosting, XGBoost) for better accuracy.

ğŸ‘©â€ğŸ’» Author

Project by Sonia Firdous. 

Special thanks to my teacher Aqsa Moiz for guidance ğŸ™
